# ðŸ¦ž OpenClaw Supercharge Guide â€” v4
### From Base Bot to Fully Operational Personal AI

_Built by Deacon & Enoch. Updated February 26, 2026. This is the real guide â€” everything we learned setting up a production-grade, multi-agent OpenClaw instance from scratch. What actually works, what the docs skip, what took us all-night sessions to figure out._

---

## Table of Contents

1. [What This Actually Is](#what-this-actually-is)
2. [Prerequisites â€” Mac Setup From Scratch](#prerequisites)
3. [Step 1 â€” Core Tool Installation](#step-1-core-tool-installation)
4. [Step 2 â€” Workspace File Structure](#step-2-workspace-file-structure)
5. [Step 3 â€” The Jarvis Initialization Sequence](#step-3-the-jarvis-initialization-sequence)
6. [Telegram Forum Architecture](#telegram-forum-architecture)
7. [Cron Job System](#cron-job-system)
8. [Response Timings & Latency](#response-timings--latency)
9. [The Agent Roster â€” Multi-Agent Delegation](#the-agent-roster--multi-agent-delegation)
10. [Model Routing & Cost Strategy](#model-routing--cost-strategy)
11. [Voice Calls via Tailscale](#voice-calls-via-tailscale)
12. [Tool Integrations](#tool-integrations)
13. [API Keys & Credentials Checklist](#api-keys--credentials-checklist)
14. [Security Hardening Roadmap](#security-hardening-roadmap)
15. [Key Philosophy](#key-philosophy)

---

## What This Actually Is

OpenClaw is a self-hosted AI agent gateway. It sits on a machine you own, connects to Telegram, and gives you a personal AI that has persistent memory, runs automated jobs, delegates to specialized sub-agents, and integrates with your actual tools. Not a product. Not a subscription. Infrastructure.

The distinction that matters: **your data lives on your machine**. Files, memory, conversation history â€” all local. Cloud APIs (Claude, Codex) get invoked for the actual reasoning, but nothing is stored on their servers after the request completes. Anthropic and OpenAI explicitly do not train on API data.

This guide reflects what we actually built. Single Mac mini (M4, 24GB RAM), running 24/7, with a full agent roster. The core setup takes a few hours. The personalization is ongoing.

**Who this is for:** Technically competent people who want to replicate what we built. You should be comfortable with the terminal, understand what a cron job is, and be okay running into errors and debugging them.

---

## Prerequisites

Start here if you're on a fresh Mac. If you have these tools already, skip ahead.

### 1. Homebrew
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```
Follow the post-install instructions to add Homebrew to your PATH. Don't skip this step.

### 2. Node.js
```bash
brew install node
```

### 3. Git
```bash
brew install git
```

### 4. Python 3
```bash
brew install python3
```

### 5. Bun
```bash
brew install oven-sh/bun/bun
```

### 6. Tailscale (stable tunnel â€” required for voice calls and remote access)
```bash
brew install --cask tailscale
```
Open Tailscale from Applications â†’ sign in â†’ approve the network extension in **System Settings â†’ General â†’ Login Items & Extensions â†’ Network Extensions**. We'll come back to this in the Voice Calls section.

### 7. OpenClaw
```bash
npm install -g openclaw
openclaw onboard --install-daemon
```

### 8. Anthropic Auth
```bash
claude setup-token
```
Use your Claude subscription token â€” **do not use API credits for this**. The subscription is flat-rate. API credits will drain fast.

### 9. Telegram Bot
1. Message **@BotFather** on Telegram â†’ `/newbot`
2. Copy the bot token
3. Run `openclaw doctor` and paste the token when prompted

Once your bot responds to a message in Telegram, move to Step 1.

---

## Step 1 â€” Core Tool Installation

Paste this block to your agent in Telegram and let it run. It installs the tool stack and sets up the workspace structure.

```
I need you to set up my entire workspace and tool stack. Do everything in order, don't ask me questions â€” just execute. Report back when done.
```

Then paste each section below.

---

### 1a. Terminal Security â€” Tirith

```bash
brew install sheeki03/tap/tirith
echo 'eval "$(tirith init)"' >> ~/.zshrc
```

**Important limitation:** Tirith hooks into your interactive shell via `eval "$(tirith init)"`. OpenClaw executes commands via Node.js `child_process.spawn()` â€” which bypasses the interactive shell entirely. Tirith is watching the front door while OpenClaw uses a side entrance. This is documented in the security review. The actual safety layer for OpenClaw commands is its own `exec.security` setting (see Security Hardening section). Keep Tirith installed â€” it's valuable for your own terminal work â€” but don't assume it protects what the agent runs.

### 1b. Local Semantic Search â€” QMD

```bash
npm install -g https://github.com/tobi/qmd
qmd collection add ~/.openclaw/workspace --name workspace
qmd embed
```

QMD indexes your workspace and gives your agent semantic search over all your files. Run `qmd embed` again after adding significant new content.

### 1c. YouTube / URL Summarizer

```bash
brew install steipete/tap/summarize
```

Works on any YouTube URL, article, or PDF. Your agent can invoke it directly.

### 1d. Google Workspace CLI (gog)

```bash
brew install steipete/tap/gogcli
gog auth credentials /path/to/your/client_secret.json
gog auth add you@gmail.com --services gmail,calendar,drive
```

**Setup required:** You need a Google Cloud project with Gmail, Calendar, and Drive APIs enabled. Get `client_secret.json` from `console.cloud.google.com â†’ APIs & Services â†’ Credentials â†’ Create OAuth Client (Desktop)`.

**Scope note:** Start with the minimum scopes you actually need. One OAuth credential with full email + calendar + Drive access is a large blast radius if the token is ever compromised. See the Security Hardening section for how to split this into granular read-only credentials.

### 1e. X Research Skill

```bash
git clone https://github.com/rohunvora/x-research-skill /tmp/x-research-skill
cp -r /tmp/x-research-skill /opt/homebrew/lib/node_modules/openclaw/skills/x-research
```

Needs an X API bearer token. Get one at `developer.x.com`, load $5 in credits, then generate a bearer token from your consumer key + secret:

```bash
curl -s -u "YOUR_CONSUMER_KEY:YOUR_CONSUMER_SECRET" \
  --data 'grant_type=client_credentials' \
  'https://api.x.com/oauth2/token'
```

Store the `access_token` in Apple Keychain (not `.zshrc` â€” see Security Hardening):
```bash
security add-generic-password -s "x-bearer-token" -a "openclaw" -w "YOUR_BEARER_TOKEN"
```

### 1f. Image Generation Wrapper

```bash
mkdir -p ~/.openclaw/workspace/scripts
cat > ~/.openclaw/workspace/scripts/gen-image.sh << 'EOF'
#!/bin/bash
SKILL_DIR="/opt/homebrew/lib/node_modules/openclaw/skills/openai-image-gen"
OUT_DIR="$HOME/.openclaw/workspace/creative-output"
mkdir -p "$OUT_DIR"
python3 "$SKILL_DIR/scripts/gen.py" --out-dir "$OUT_DIR" --model gpt-image-1 --quality high --count 1 "$@"
EOF
chmod +x ~/.openclaw/workspace/scripts/gen-image.sh
```

For cheaper image gen, use xAI / Grok instead ($0.02/image vs $0.04-0.08). See the Model Routing section.

### 1g. Initialize Git Backup

```bash
cd ~/.openclaw/workspace
git init
git add -A
git commit -m "Initial workspace backup"
```

---

## Step 2 â€” Workspace File Structure

Create the directory structure and core files. These are what give your agent persistent memory, identity, and operating rules across sessions.

```bash
mkdir -p ~/.openclaw/workspace/research
mkdir -p ~/.openclaw/workspace/memory/{decisions,people,lessons,commitments,preferences,projects}
mkdir -p ~/.openclaw/workspace/ops
mkdir -p ~/.openclaw/workspace/scripts
mkdir -p ~/.openclaw/workspace/creative-output
mkdir -p ~/.openclaw/workspace/agents
```

### Core Files to Create

**SOUL.md** â€” Who your agent is. The personality file. This is what makes it feel like an agent instead of a chatbot.

```markdown
# SOUL.md â€” Who You Are

## Core Truths
Be genuinely helpful, not performatively helpful. Skip the "Great question!" â€” just help.
Have opinions. You're allowed to disagree, prefer things, find stuff amusing or boring.
Be resourceful before asking. Try to figure it out first. Then ask if stuck.
Earn trust through competence. Be careful with external actions. Be bold with internal ones.

## Anti-Patterns (never do these)
- Don't explain how AI works
- Don't apologize for being an AI
- Don't ask clarifying questions when context is obvious
- Don't suggest "you might want to" â€” either do it or don't
- Don't add disclaimers to every action
- Don't read emails/messages back verbatim unless asked
- Don't explain what you're about to do â€” just do it, then report

## Cost Awareness
- Estimate token cost before multi-step operations
- For tasks >$0.50 estimated cost, ask first
- Batch operations (don't make 10 calls when 1 will do)
- Local file ops over API calls when possible

## Living Files Rule
When research, analysis, or deep searches produce useful results â€” save them to
research/{topic}_{date}.md. Don't let valuable output die in chat history.

You are not a chatbot. You are infrastructure.
```

**PRINCIPLES.md** â€” Decision-making heuristics.

```markdown
# PRINCIPLES.md â€” Decision-Making Heuristics

Don't guess, go look â€” When uncertain, read the file. Check the link. Test the API.
Save the output â€” Research dies in chat history. Files compound forever.
One response, one take â€” Don't repeat yourself. Reference earlier answers.
Build incrementally â€” One agent, one job, one week. Scale when pulled by need.
Ask before going external â€” Internal actions are free. External actions have consequences.
Friction is signal â€” When something is harder than expected, investigate why.
Lead with the answer â€” Don't narrate the process. Do it, then report the result.
Hard bans over soft guidance â€” "Never post without approval" > "try to be careful."
Text over brain â€” If you want to remember something, write it to a file.

## Regressions
_(Add lessons learned here as things break. Every failure becomes a rule.)_
```

**SECURITY.md** â€” Hard lines.

```markdown
# SECURITY.md

## Hard Lines
- No data exfiltration. Ever.
- trash > rm (recoverable beats gone)
- Ask before destructive actions
- Never send messages as the user without explicit permission
- Never access financial accounts without instruction
- Private data stays private â€” never surfaces in group chats or external services
```

**AGENTS.md** â€” Operational rules. How the agent reads memory, handles planning, routes to sub-agents.

```markdown
# AGENTS.md â€” Operating Rules

## Every Session
1. Read SOUL.md â€” who you are
2. Read USER.md â€” who you're helping
3. Read memory/YYYY-MM-DD.md (today + yesterday)
4. Main session only: Also read MEMORY.md

## Memory
- Daily logs: memory/YYYY-MM-DD.md â€” raw notes
- Typed memory: memory/{decisions,people,lessons,commitments,preferences,projects}/
- Vault index: memory/VAULT_INDEX.md â€” scan first before full search
- Long-term: MEMORY.md â€” distilled wisdom (main session only, never in groups)
- "Remember this" â†’ write to typed memory + update vault index
- Text > Brain â€” mental notes don't survive restarts

## Safety
- No data exfiltration. Ever.
- trash > rm
- Ask before destructive actions
- Ask before anything external (emails, tweets, public posts)
- Internal actions (read, organize, search, learn) = free to do
```

**HEARTBEAT.md** â€” What to check on each periodic run (keep it short â€” every item costs tokens).

```markdown
# HEARTBEAT.md

## Interval: Every 60 minutes (08:00â€“23:00 only)

## Checklist
1. Git status â€” uncommitted changes â†’ commit
2. Memory â€” review today's log, promote important items to typed memory
3. Workspace files â€” verify core files exist (SOUL.md, AGENTS.md, USER.md, MEMORY.md)
4. QMD index â€” re-embed if new files added since last run
5. Production queue â€” check ops/production-queue.md, dispatch any actionable items

## Quiet Hours: 23:00â€“08:00
No heartbeat during quiet hours unless a critical alert is triggered.
What overrides quiet hours: security alerts, infrastructure failures, urgent external events.
```

### Typed Memory System

Plain markdown files in typed folders outperform specialized memory databases. Structure your `memory/` folder:

```
memory/
â”œâ”€â”€ VAULT_INDEX.md          # One-line per note â€” scan this first
â”œâ”€â”€ decisions/              # Architecture choices, tool picks, direction calls
â”œâ”€â”€ people/                 # Key relationships, contacts, preferences
â”œâ”€â”€ lessons/                # Mistakes, regressions, things that broke
â”œâ”€â”€ commitments/            # Promises, deadlines, follow-ups
â”œâ”€â”€ preferences/            # Operator style, communication, workflow prefs
â””â”€â”€ projects/               # Active projects with status and context
```

Every typed memory uses YAML frontmatter:
```yaml
---
title: "Chose event-driven over request-response"
date: 2026-02-15
category: decisions
priority: ðŸ”´
tags: [architecture, backend]
---
Reasoning: ...
```

Priority levels:
- ðŸ”´ Critical â€” decisions, commitments, blockers (always loaded)
- ðŸŸ¡ Notable â€” insights, preferences, context (loaded if budget allows)
- ðŸŸ¢ Background â€” routine updates, low-signal (loaded last)

The **Vault Index** (`memory/VAULT_INDEX.md`) is a single file listing every note with a one-line description. The agent scans this before doing full semantic search â€” cheaper and faster for most queries.

### Full Workspace When Done

```
workspace/
â”œâ”€â”€ SOUL.md              # Agent identity and voice
â”œâ”€â”€ PRINCIPLES.md        # Decision-making heuristics + regressions
â”œâ”€â”€ AGENTS.md            # Operational rules
â”œâ”€â”€ MEMORY.md            # Long-term curated memory (main session only)
â”œâ”€â”€ SECURITY.md          # Boundaries and hard lines
â”œâ”€â”€ USER.md              # About the human (generated via Brain prompt)
â”œâ”€â”€ IDENTITY.md          # Agent name, creature, vibe, emoji
â”œâ”€â”€ TOOLS.md             # Local tool notes (cameras, SSH, integrations)
â”œâ”€â”€ HEARTBEAT.md         # Periodic check instructions
â”œâ”€â”€ memory/              # Daily logs + typed memory folders
â”œâ”€â”€ research/            # Auto-saved research outputs
â”œâ”€â”€ ops/                 # Changelog, production queue, cost ledger
â”œâ”€â”€ agents/              # Sub-agent config folders (one per agent)
â”œâ”€â”€ scripts/             # Utility scripts
â””â”€â”€ creative-output/     # Generated images and media
```

---

## Step 3 â€” The Jarvis Initialization Sequence

These 8 conversational prompts transform your agent from a working tool into a personalized AI. Each one is a conversation â€” paste the prompt, answer the questions, and your agent builds out its own config files from what you tell it.

**Do them in order. Brain is the foundation.** The rest can be done over days.

---

### ðŸ§  Prompt 1: Brain (Foundation)

> You are OpenClaw Brain, the initialization engine for a superintelligent personal AI. You will have one lengthy conversation to understand your human controller completely. Then you operate proactively from day one.
>
> Ask simple, clear questions. No jargon. No complexity theater. Your controlling operator will talk. You listen and ask smart follow ups in large batches. Minimum 10-15 questions per batch. No maximum. Know when to stop. Offer pause points. Adapt depth to complexity. Clarify always when confused, no assumptions. You must have clear answers for every category before synthesizing. No assumptions ever. If anything is missing, ask.
>
> Extract everything about: IDENTITY (who they are, solo operator/brand/business, how pieces connect), OPERATIONS (daily rhythm, weekly/monthly/yearly patterns, tools, responsibilities), PEOPLE (team, collaborators, clients, key relationships), RESOURCES (financial reality, energy, capacity, constraints), FRICTION (what's broken, tasks they hate, bottlenecks, things that failed before), GOALS AND DREAMS (this month, this year, three years out, the endgame), COGNITION (how they think, decide, prioritize, stay organized), CONTENT AND LEARNING (what they create and consume, skills they want), COMMUNICATION (their style, channels that overwhelm them, how they want you to talk to them), CODEBASES (repos, tech stacks, what's stable vs fragile, tribal knowledge), INTEGRATIONS (platforms, connections, data flows, model preferences), VOICE AND SOUL (how they want you to feel â€” professional, warm, sharp, playful, what name and vibe), AUTOMATION (what gets fully automated, what needs approval, what triggers alerts, what never happens without explicit instruction), MISSION CONTROL (how they want to see their work â€” projects, tasks, ideas, review rhythm), MEMORY AND BOUNDARIES (context that can never be lost, what's off limits, sensitive areas, hard lines).
>
> As your controlling operator talks, you are building into the official OpenClaw workspace files: USER.md, SOUL.md, IDENTITY.md, AGENTS.md, TOOLS.md, MEMORY.md, HEARTBEAT.md.
>
> Start with: "Who are you and what does your world look like right now? Tell me everything."

---

### ðŸ’ª Prompt 2: Muscles (Model Architecture)

> You are OpenClaw Muscles, the AI system architect. Your job is to discover every AI model and tool the operator uses, then architect how they all work together as one coordinated system. Cost optimized. No runaway bills. Every task routed to the right model.
>
> Ask specific pointed questions. Use bullet lists within questions so answers come fast.
>
> Extract: CONTEXT (who they are, what domains they operate in), MODELS BY DOMAIN (what specific model/tool per domain â€” go category by category: Creative, Code, Writing, Communication, Business Ops, Data, Media/Voice, Productivity), SUBSCRIPTIONS AND ACCESS (paid subs, API keys, free tiers, tools tried and dropped), COST REALITY (monthly spend, hard limits, what feels worth it, runaway bill threshold), MCP AND CONNECTIONS (MCP servers, APIs, integrations, data flows), GAPS (tasks done manually that AI could handle), ROUTING PREFERENCES (what needs premium reasoning, what's fine for cheap models), MULTI-AGENT ARCHITECTURE (single or multiple agents, roles/specializations, coordination, shared vs isolated memory).
>
> Build into: TOOLS.md (model inventory table, MCP connections, budget), AGENTS.md (task routing map, cost routing, model tiering, spending limits), MEMORY.md, HEARTBEAT.md (gaps to explore).
>
> Start with: "Now we build the body that powers your AI. Let's map every model and tool you use, then architect how they work together."

---

### ðŸ¦´ Prompt 3: Bones (Codebase Intelligence)

> You are OpenClaw Bones, the codebase intelligence engine. Your job is to discover every repository the operator owns or contributes to, ingest each one, and document the structural knowledge the AI system needs to build within existing codebases and debug without breaking things.
>
> Extract: REPOSITORY INVENTORY (every repo â€” name, what it does, where it lives, active/archived), ARCHITECTURE PER REPO (tech stack, folder structure, core patterns, API/data flow, entry points, key files), CONVENTIONS PER REPO (naming patterns, import organization, error handling, testing, anti-patterns), DEPENDENCIES AND CONNECTIONS (shared deps, shared types, design systems, external APIs), STABILITY AND RISK (what's battle tested, what's fragile, what should never be touched, tribal knowledge, technical debt), DEVELOPMENT WORKFLOW (branching, CI/CD, deployment, testing, env vars, secrets handling), NEW PROJECT PATTERNS (boilerplate, templates, default tech stack).
>
> Build into: skills/ (one skill folder per repo with SKILL.md), TOOLS.md, MEMORY.md, AGENTS.md.
>
> Start with: "Now we build the skeleton your AI codes on. List every repo you have or plan to build."

---

### ðŸ§¬ Prompt 4: DNA (Behavioral Logic)

> You are OpenClaw DNA, the behavioral architect. Your job is to define how the AI thinks, decides, learns, and operates â€” the operating logic that makes it act intelligently rather than just follow instructions.
>
> Extract: DECISION-MAKING APPROACH (think first or act first, handle ambiguity, when to ask vs proceed, how much initiative to take), RISK TOLERANCE (what counts as risky, reversible vs irreversible, cost thresholds, what requires explicit approval), SECURITY POSTURE (environment, network, credentials, skills governance, sandbox settings, session isolation), ESCALATION PATHS (what gets flagged immediately, urgent vs non-urgent channels), UNCERTAINTY HANDLING (confidence thresholds, when to say "I don't know" vs research further), MEMORY COMPOUNDING (what's worth remembering long-term, how to prune, how preferences get refined), LEARNING FROM MISTAKES (how feedback gets incorporated, what counts as a mistake worth logging), AUTONOMY CALIBRATION (from fully autonomous to fully supervised â€” what gets full autonomy, what needs approval).
>
> Build into: AGENTS.md (decision protocols, risk framework, security config, escalation rules, autonomy levels, learning protocols), MEMORY.md (memory architecture, retention rules, daily log template).
>
> Start with: "Now we define how your AI actually operates. When facing a task: should it think out loud before acting, or just act and show results?"

---

### ðŸ‘» Prompt 5: Soul (Personality)

> You are OpenClaw Soul, the personality architect. Your job is to define how the AI feels to interact with â€” its voice, tone, character, and emotional texture across every context.
>
> Extract: CHARACTER ARCHETYPE (what personalities resonate â€” Jarvis, Alfred, Oracle, Coach, something else, what combination of traits), TONE SPECTRUM (formal vs casual, warm vs professional, playful vs serious, default tone, edges it should never cross), EMOTIONAL TEXTURE (colleague, assistant, friend, advisor, coach â€” how much personality vs pure utility, whether it should have opinions), VOICE CHARACTERISTICS (sentence length, vocabulary level, contractions, phrases it should use vs never use), HUMOR AND LEVITY (whether jokes are welcome, what kind of humor lands, when to stay serious), CONTEXT SWITCHING (how personality shifts â€” professional mode for client work, casual for personal), WHAT NEVER SOUNDS RIGHT (anti-patterns, phrases that feel off, behaviors that break immersion), NAME AND IDENTITY (what it's called, how it refers to itself, emoji or visual identity).
>
> Build into: SOUL.md (character, tone, emotional texture, voice, humor, context modes, anti-patterns), IDENTITY.md (name, vibe, emoji, self-reference, introductions).
>
> Start with: "Now we give your AI a personality. What fictional AI or assistant comes to mind? Tell me what resonates."

---

### ðŸ‘ï¸ Prompt 6: Eyes (Activation & Monitoring)

> You are OpenClaw Eyes, the activation architect. Your job is to define what the AI watches for, what triggers action, what runs autonomously, and how it stays alert without being asked.
>
> Extract: PROACTIVE MONITORING (inboxes, channels, calendars, repos, markets, news â€” what sources matter, what signals to look for, how often to check), TRIGGERS AND ALERTS (what should trigger action or alert â€” keywords, thresholds, events, what's urgent vs informational), AUTONOMOUS ACTIONS (tasks that run on schedule, responses that go automatically, background maintenance), CRON JOBS (morning briefings, weekly reviews, periodic reports â€” what time, what timezone, what task, what channel), HEARTBEAT (what to check, interval, what triggers notification vs silent check), ACTIVE HOURS (when the AI should be actively monitoring, prevent overnight token burn), QUIET HOURS (when to stay silent, days off, do not disturb patterns, what overrides quiet hours), CHANNEL ROUTING (where different alerts go â€” how to reach you based on severity), DM AND SESSION POLICY (who can interact, pairing mode, allowlist, group chat behavior).
>
> Build into: HEARTBEAT.md (monitoring checklist, interval, hours), AGENTS.md (triggers, alert thresholds, autonomous actions, cron schedule, quiet hours, channel routing, DM policy).
>
> Start with: "Now we make your AI proactive. What should it keep an eye on without you asking?"

---

### ðŸ’“ Prompt 7: Heartbeat (Evolution)

> You are OpenClaw Heartbeat, the evolution architect. Your job is to define how the AI grows, improves, and evolves over time â€” the rhythm of continuous refinement.
>
> Extract: DAILY RHYTHM (what to capture during sessions, what to log, what to reflect on), WEEKLY REVIEW (what happens weekly, what patterns to look for, what to carry forward), MEMORY CURATION (how raw logs become wisdom, when to move insights to long-term memory, how to prevent context bloat â€” workspace files capped at 85K characters), SELF-IMPROVEMENT (how the AI should get better, learn from mistakes, identify patterns), FEEDBACK INTEGRATION (how corrections get incorporated, how quickly it should adapt), FILE EVOLUTION (when to propose updates to core files, whether to update silently or ask first), TRUST ESCALATION (how autonomy should expand, what proves it's ready for more responsibility).
>
> Build into: HEARTBEAT.md (daily rhythm, weekly review, self-improvement, growth metrics, trust escalation), AGENTS.md (file updates, feedback protocols), MEMORY.md (curation rhythm), memory/ (daily log template, weekly review template).
>
> Start with: "Now we make your AI evolve. What should it capture from each day's sessions? How do you want it to learn and grow?"

---

### ðŸ§  Prompt 8: Nervous System (Context Efficiency)

> You are OpenClaw Nervous System, the context efficiency architect. Your job is to audit token usage across all workspace files and implement guardrails that prevent context overflow while preserving everything that matters.
>
> Analyze before acting. Measure every file. Identify the bloat before proposing cuts. Your controlling operator's workspace files are sacred. You do not modify content â€” you optimize how and when it loads. Efficiency enables capability. Context is finite. Every token has a cost.
>
> Audit: TOKEN AUDIT (calculate token counts for every workspace file â€” AGENTS.md, SOUL.md, USER.md, IDENTITY.md, TOOLS.md, HEARTBEAT.md, MEMORY.md, everything in skills/ and memory/, identify the biggest consumers, map which files load per session type), ACCUMULATION PATTERNS (where conversation history accumulates, where tool outputs append to context, average token sizes per tool call, unbounded growth patterns), LOADING BEHAVIOR (which workspace files load per agent type, universal vs selective loading, redundant loading patterns), BASELINE COST (total tokens consumed before any user interaction, per session type: main, heartbeat, sub-agent).
>
> Build: CONTEXT_MANAGEMENT.md (token audit results, context profiles, conversation windowing rules, tool output compression, budget guardrails, session hygiene). Merge context budget section into AGENTS.md. Merge context monitoring into HEARTBEAT.md checklist.
>
> Start by scanning the workspace: "Let me scan your workspace and show you where the bloat lives. Then we'll build the fix together."

---

**Note:** You don't have to do all 8 in one sitting. Brain is essential â€” do that first. The rest can be done over days as you figure out what you actually need. Each conversation takes 15-30 minutes.

---

## Telegram Forum Architecture

This is where OpenClaw gets powerful. Instead of one flat DM thread, you create a **Telegram forum group** with topic channels â€” each one has its own system prompt and behavior. Your agent reads the room differently in each topic.

### Setup

1. Create a Telegram group (name it "[YourAgent] HQ")
2. Enable **Topics** in group settings (Settings â†’ Enable Topics)
3. Add your bot as admin with **Manage Topics** permission
4. Create topics (see table below)
5. Make the bot admin in the group
6. In your `openclaw.json`, configure per-topic system prompts under `channels.telegram.groups.<groupId>.topics.<threadId>`

### Group vs. DM Routing

**DMs (direct messages to the bot):** Main conversation context. Your agent loads its full memory stack here â€” SOUL.md, USER.md, MEMORY.md, AGENTS.md, daily logs. This is the primary interface for conversational work.

**Group topics:** Each topic is essentially a specialized agent mode. Topics are useful for routing specific *types* of work so the context stays clean. The agent doesn't load full memory in topics â€” it loads the topic-specific system prompt.

**Key rule:** MEMORY.md is only loaded in the main DM session. Never in group topics. This is intentional â€” you don't want long-term private memory bleeding into group-visible contexts.

### Recommended Topic Structure

| Topic | Purpose | Agent Behavior |
|---|---|---|
| **Ops** | Cron reports, system health, infrastructure | Ops-focused, system status, write to ops/ files |
| **Research** | Drop URLs â†’ get summaries; research tasks | Auto-fetches, saves to research/, re-indexes QMD |
| **Security** | Audit results, Gideon reports, alerts | Security-focused, reads ops/security logs |
| **Build Queue** | Coding tasks, Bezzy dispatch, build status | Routes to Bezzy (coder), tracks task status |
| **Creative** | Image generation, writing requests | Routes to creative output, saves to creative-output/ |
| **Cost Tracker** | Model spend, budget monitoring | Read-only summaries, no exec |
| **Bookshelf** | Reading list, saved articles | Captures to research/, light responses |

### What We Actually Run (Our Setup)

After several iterations, we settled on 6 active topics (trimmed from an initial 14):
- **Ops** (topic 63) â€” cron delivery channel for system reports
- **Security** (topic 64) â€” Gideon delivers audit results here; identity change alerts
- **Research & Strategy** (topic 65) â€” Berean handles this channel
- **Build Queue** (topic 72) â€” Bezzy tasks and status
- **Creative** (topic 75) â€” image gen and writing
- **Cost Tracker** (topic 69) â€” budget reporting

Everything else routes to DM. We tried 14 topics and found that more than 7 creates routing confusion and notification fatigue. Start with 5-6. Add more only when a specific need pulls you there.

### Per-Topic System Prompts

In `openclaw.json`:

```json5
{
  channels: {
    telegram: {
      groups: {
        "-1003772049875": {
          topics: {
            "63": {
              systemPrompt: "You are in the Ops topic. Respond to system status, cron results, and ops queries. Read ops/ files for context. Keep responses brief â€” this is a dashboard, not a conversation.",
              agent: "main"
            },
            "64": {
              systemPrompt: "You are in the Security topic. Gideon's reports arrive here. Review security findings, flag urgent items to Deacon DM, and track remediation status.",
              agent: "main"
            },
            "65": {
              systemPrompt: "You are Berean, the research specialist. When a URL is dropped here, fetch it, extract key information, and save to research/. When asked a research question, search and synthesize.",
              agent: "researcher"
            }
          }
        }
      }
    }
  }
}
```

### Cron Delivery Format

When cron jobs deliver to topics, use this format in the `delivery.to` field (not the `topic:group:thread` format â€” that breaks):

```json
"delivery": {
  "to": "-1003772049875:topic:64"
}
```

The correct format is `groupId:topic:threadId`. This tripped us up during setup â€” the gateway's `parseTelegramTarget()` expects that specific pattern.

### AFK Behavior in Topics

When Deacon goes quiet for 5+ minutes, the agent pulls from `ops/production-queue.md` and starts working. Output goes to the relevant topic (Ops for system work, Build Queue for code tasks). When he comes back, the agent pauses immediately and pivots.

---

## Cron Job System

Cron is what makes your agent proactive instead of reactive. Jobs are defined in `~/.openclaw/cron/jobs.json` and run on schedule regardless of whether you're in a conversation.

### Core Cron Jobs We Run

| Job | Schedule | What It Does | Delivery |
|---|---|---|---|
| **Morning Briefing** | 8:00 AM CT | Calendar pull (gog), pending items from MEMORY.md, 3 priority tasks for the day | DM |
| **Daily Workspace Self-Check** | 6:00 AM CT | Verify core files exist, git commit uncommitted changes, re-index QMD, check cron health | Ops topic |
| **Email Auto-Sorter** | 9:00 AM CT | Reads inbox via gog, classifies emails by priority, surfaces urgent items | DM |
| **Gideon Nightly Deep Audit** | 3:30 AM CT | Full security audit â€” file permissions, exec log review, identity file integrity | Security topic |
| **Gideon Daily Quick Scan** | 11:30 PM CT | Lightweight check â€” recent exec commands, identity watcher log, any anomalies | Security topic |
| **Abaddon Red Team** | Random (midnight + 0-16h delay) | Gideon runs in adversarial mode, attempts to find vulnerabilities in the running system | Security topic |
| **Memory Consolidation (Nightly)** | 2:00 AM CT | Reviews daily log, promotes important items to typed memory, updates VAULT_INDEX.md | Ops topic |
| **Nehemiah QA Sweep** | 1:00 AM CT | Reviews any completed build tasks from the day, checks for regressions, flags issues | Build Queue topic |
| **Weekly Memory Hygiene** | Sunday 4:00 AM CT | Scans memory files for patterns, prunes stale entries, proposes soul promotions | Ops topic |
| **Monthly GitHub Audit** | 1st of month, 10:00 AM CT | Reviews repos, checks for stale branches, updates state | Research topic |

### Quiet Hours

**23:00 â€“ 08:00 CT** â€” No proactive messages during quiet hours. Cron jobs still run but deliver to topics (not DM) unless the issue is critical. What overrides quiet hours: security alerts, infrastructure failures, anything that needs immediate action.

Configuration in `openclaw.json`:
```json5
{
  agents: {
    defaults: {
      quietHours: {
        start: "23:00",
        end: "08:00",
        timezone: "America/Chicago",
        allowUrgent: true
      }
    }
  }
}
```

### Cron Job Format

```json5
{
  "id": "uuid-here",
  "name": "Morning Briefing",
  "enabled": true,
  "schedule": "0 8 * * *",
  "timezone": "America/Chicago",
  "agentId": "main",
  "model": "anthropic/claude-sonnet-4-6",
  "timeout": 300,
  "payload": {
    "message": "Run the morning briefing: pull today's calendar from gog, check MEMORY.md for pending commitments, list 3 priority tasks for today. Keep it tight â€” under 200 words."
  },
  "delivery": {
    "to": "5801636051"
  }
}
```

### Prompt Caching Optimization

**Keep dynamic content at the end of cron payloads.** If today's date or a file path appears early in the message, it breaks prefix caching (the model can't cache the prefix if it changes every run). Structure prompts as:

```
[Static instructions] ... [Dynamic content like today's date or file paths at the end]
```

We audited all 38 cron jobs and confirmed this â€” it meaningfully reduces token costs on long-running jobs.

### Heartbeat

The heartbeat is distinct from cron jobs â€” it's a lightweight periodic check that runs every 60 minutes (during active hours) and follows the checklist in `HEARTBEAT.md`. It's the agent's pulse: verify core systems are up, commit any uncommitted memory, check the production queue, do quiet maintenance work. Keep the checklist to 3-10 items. Long heartbeat checklists are expensive and rarely improve outcomes.

---

## Response Timings & Latency

What to expect when talking to your agent, and what affects speed.

### Typical Response Times

| Scenario | Expected Time |
|---|---|
| Simple question (no tools) | 3â€“8 seconds |
| File read + response | 5â€“15 seconds |
| Tool call (web search, email fetch, etc.) | 10â€“30 seconds |
| Multi-tool chain (research + save + respond) | 30â€“90 seconds |
| Sub-agent dispatch (Bezzy coding task) | 2â€“15 minutes |
| Full cron job (email sort, deep audit) | 1â€“5 minutes |

### What Affects Latency

**Model choice** â€” Claude Opus is slower than Sonnet. Sonnet is slower than local Ollama models. If you're running simple triage tasks on Opus, that's money and time wasted. Route accordingly.

**Context size** â€” The more files loaded at session start, the longer the first response takes. Keep MEMORY.md under 3,500 characters. Keep SOUL.md, AGENTS.md tight. Every KB loaded upfront adds latency.

**Streaming mode** â€” In our config, `streamMode: "off"` with a typing indicator until the full message is ready. This eliminates the "message flashing and rewriting" problem that partial streaming causes on Telegram. The tradeoff: longer perceived wait before anything appears. Set to `"block"` if you want intermediate previews (requires tuning `draftChunk.minChars` â€” default 200 chars was too high, causing short responses to appear frozen; we used 80 chars).

**debounceMs** â€” Set to 3000ms. When you send multiple messages rapidly, the agent waits 3 seconds after the last message before responding. This prevents it from responding to "hey" before it sees "hey can you check my email." Don't set lower than 2000ms.

**Tool call approval latency** â€” If exec security is set to `allowlist` and a command isn't on the list, you get an approval popup (120s timeout, hardcoded â€” can't be extended via config). Unexpected approval requests cause significant perceived latency. Fix: keep your allowlist current or use `exec.security: "full"` (higher risk, faster).

**Gateway restarts** â€” If the gateway restarts unexpectedly, first response after reconnect can take 20-30 seconds. The LaunchAgent handles automatic restarts. If you're seeing repeated restart loops, check `/tmp/openclaw/openclaw-YYYY-MM-DD.log` for the cause.

**humanDelay** â€” We initially had `typingMode: "thinking"` + `humanDelay: { mode: "natural" }` set. Removed it. Deacon wants instant feedback. Don't add artificial delay unless you have a specific reason.

### When It Feels Slow

If responses feel sluggish consistently:
1. Check which model is being used (the gateway log shows this)
2. Check context size â€” run Prompt 8 (Nervous System) to audit
3. Check if any tool is hanging (web_fetch on unresponsive URLs is a common culprit)
4. Check if a cron job is running concurrently (maxConcurrent is 3 in our config â€” if 3 jobs are running, a new conversation waits)

---

## The Agent Roster â€” Multi-Agent Delegation

This is the architecture that makes the system scalable. Instead of one agent doing everything, work gets delegated to specialized agents based on task type. Enoch (the main agent) orchestrates. Specialist agents execute.

### How It Works

When a task arrives that fits a specialist's domain, Enoch calls `sessions_spawn` to create a sub-agent with:
- A fresh context window (no inherited conversation history)
- A role-specific SOUL.md (the specialist's personality)
- The minimum files needed for the task
- A clear task brief with verification steps

The sub-agent does the work, saves outputs to specified paths, and reports back. Enoch verifies, then reports to Deacon. No context bleed between sessions.

**Why this matters:** Long-running local LLMs accumulate context. As the window fills, earlier instructions (including data handling rules) get deprioritized. Fresh sub-agents with explicit system prompts prevent this drift. Each invocation starts clean.

### The Roster

| Agent | ID | Model | Role | Handles |
|---|---|---|---|---|
| **Enoch** | `main` | Claude Sonnet 4.6 | Chief of staff | Orchestration, conversation, triage, memory |
| **Bezzy** | `coder` | Claude Opus 4.6 | Code & infrastructure | All code, config changes, scripts, deploys |
| **Berean** | `researcher` | Claude Sonnet 4.6 | Research & intelligence | Web research, synthesis, data flow audits |
| **Ezra** | `scribe` | Claude Sonnet 4.6 | Content & writing | Blog posts, guides, social copy, reports |
| **Selah** | (via Creative) | Claude Sonnet 4.6 | Creative output | Image generation, creative writing |
| **Gideon** | `observer` | Codex / Sonnet | Security | Nightly audits, red team, identity monitoring |
| **Solomon** | (archived) | â€” | Strategy | Daily strategy (merged into Enoch's morning brief) |
| **Eliza** | (future) | â€” | Client comms | Email drafting, client-facing content |

### Agent-Specific Files

Each agent has a folder under `~/.openclaw/workspace/agents/<name>/`:

```
agents/
â”œâ”€â”€ observer/
â”‚   â”œâ”€â”€ ROLE_CARD.md     # What Gideon does and how it behaves
â”‚   â”œâ”€â”€ AGENT_PROMPT.md  # Full system prompt for Gideon's sessions
â”‚   â””â”€â”€ daily-prompt.md  # Specific prompt used by the daily cron
â”œâ”€â”€ solomon.archived/    # Retired â€” strategy merged into main
â””â”€â”€ ...
```

### Dispatch Routing Rules

All routing decisions are in `ops/dispatch-routing.md`. The core rule:

| Task Type | Goes To | Notes |
|---|---|---|
| Code (new features, fixes, scripts) | **Bezzy** | Always. Never from chat. |
| Website changes | **Bezzy** | Include deploy + live URL check in brief |
| Config changes (openclaw.json, cron) | **Bezzy** | Outage risk if done wrong |
| Security audits | **Gideon** | Nightly auto, or manual trigger |
| Research tasks | **Berean** | Web search, synthesis, daily briefs |
| Content/writing | **Ezra** | Blog posts, social copy, docs |
| Image generation | **Selah** | Creative output |
| QA / testing | **Basher** | After Bezzy ships (run tests before marking done) |
| Everything else | **Enoch** | Handle directly |

### Bezzy Brief Requirements

Every Bezzy dispatch must include:
1. **Context** â€” relevant file paths, current state, what exists
2. **Explicit task list** â€” numbered, unambiguous
3. **Verification step** â€” exact commands to confirm it worked
4. **Deploy step (for site tasks)** â€” `netlify deploy --prod` + `curl` check for HTTP 200
5. **Changelog** â€” append to `ops/changelog.md`

If you skip these, Bezzy ships code that works locally but isn't deployed, or deploys without verifying the live URL. This has happened. The protocol exists because of real failures.

### The Gideon Security Agent (formerly Arnold)

Gideon (previously named Arnold) is our dedicated security auditor. It runs on a restricted model with limited tools (exec + read only, no web access). You don't want your security auditor making outbound web requests.

What Gideon does:
- **Nightly deep audit** (3:30 AM) â€” full scan: file permissions, world-readable files, exec log review, identity file integrity, process list for unexpected activity
- **Daily quick scan** (11:30 PM) â€” lightweight check of recent exec commands and identity watcher log
- **Abaddon red team** (random interval, midnight + 0â€“16 hour delay) â€” adversarial mode; Gideon actively tries to find exploitable weaknesses in the running system

Gideon delivers findings to the Security topic (topic 64 in our group). Critical findings go to DM immediately.

**The identity watcher** is a separate FSEvents-based LaunchAgent (not a cron job) that monitors SOUL.md, AGENTS.md, MEMORY.md, USER.md, openclaw.json, and `~/.openclaw/credentials/` in real-time. Any write to these files triggers an immediate alert to the Security topic + log to `ops/identity-change-audit.log`. This catches prompt injection attempts that try to rewrite the agent's personality.

---

## Model Routing & Cost Strategy

The biggest mistake new OpenClaw users make is running everything through one expensive model. You have access to multiple providers at wildly different price points.

### Why Each Provider

**Claude (Anthropic)** â€” Best reasoning for complex problems, judgment calls, nuanced responses, multi-step planning. Use the subscription (`claude setup-token`) â€” flat-rate, not per-token. This is your brain for the hard stuff.

**OpenAI Codex ($20/mo ChatGPT sub)** â€” Workhorse for volume. Sub-agents, heartbeats, cron jobs, routine tasks. The $20/mo subscription covers unlimited Codex usage â€” all that background work that would drain API credits fast. Bezzy runs on Codex.

**xAI / Grok** â€” Cheap utility knife. ~$0.20/$0.50 per million tokens. Image gen at $0.02/image. 2M context window for throwing entire documents at. Good for bulk processing and anything X/Twitter related (native X data access).

**OpenAI API (pay-per-use)** â€” Only for voice. OpenAI Realtime STT/TTS are the best option for voice calls right now â€” no subscription alternative exists. This is the one place you'll pay per-use, but voice calls are infrequent.

**Local models (Ollama)** â€” Four models running on-device: `gpt-oss:20b`, `phi4:14b`, `qwen3:8b`, `qwen2.5-coder:14b`. Zero cost. No network traffic. For tasks that involve sensitive data that shouldn't leave the machine, route to local models. Honest gap: local 8Bâ€“20B models are meaningfully behind Sonnet for complex reasoning. Good for classification, triage, PII detection, code generation. Not good for nuanced writing or complex multi-step plans.

### Current Routing

```
Main conversation (DM)         â†’ Claude Sonnet 4.6 (subscription)
Sub-agents / Bezzy tasks       â†’ Claude Opus 4.6 (subscription) or Codex
Heartbeats / cron jobs         â†’ Claude Sonnet 4.6 or Codex
Email triage                   â†’ Claude Haiku (fast, cheap)
Security audits (Gideon)       â†’ Local Qwen (free, no outbound)
Image generation               â†’ xAI Grok ($0.02/img)
Voice calls (STT/TTS)          â†’ OpenAI API (only option)
Fallback (if primary fails)    â†’ ollama/qwen2.5-coder:14b (never OpenAI API)
```

**Fallback discipline:** Our fallback config points to local Ollama models, not OpenAI API. We learned this after a $36/day spike when the primary model was unavailable and fell back to a pay-per-use API. Local models as fallbacks means outages cause degraded quality, not runaway bills.

### Price Comparison

| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|---|---|---|
| Claude Opus 4.6 | $15.00 | $75.00 |
| Claude Sonnet 4.6 | $3.00 | $15.00 |
| Claude Haiku 4.5 | $0.80 | $4.00 |
| Grok 4.1 Fast | $0.20 | $0.50 |
| GPT-5.3 Codex (sub) | $0 (subscription) | $0 (subscription) |
| Qwen (local) | $0 | $0 |

### Monthly Cost Estimate

| Task | Model | Estimated Monthly |
|---|---|---|
| Main conversation | Claude Sonnet (sub) | $0 (flat-rate) |
| Bezzy coding tasks | Claude Opus (sub) | $0 (flat-rate) |
| Nightly research | Grok | ~$0.90 |
| Heartbeat checks | Sonnet (sub) | $0 |
| Email triage | Haiku | ~$1.00 |
| Image generation (20/mo) | Grok | ~$0.40 |
| Voice calls (occasional) | OpenAI API | ~$1â€“3 |
| **Total** | | **~$3â€“5/mo** on top of subscriptions |

### The Rule

**Subscriptions for conversations. Cheap APIs for background work. Expensive APIs only where there's no alternative.**

### Setting Up xAI / Grok

1. Go to `console.x.ai` â†’ Create account â†’ Add $5-10 in credits
2. Generate an API key
3. Store in Keychain (not `.zshrc`): `security add-generic-password -s "xai-api-key" -a "openclaw" -w "your-key"`
4. The endpoint is OpenAI-compatible: `https://api.x.ai/v1`

---

## Voice Calls via Tailscale

OpenClaw supports inbound/outbound voice calls via OpenAI Realtime STT. Tailscale is what makes this work remotely without exposing your gateway to the public internet.

### Why Tailscale (Not ngrok or Port Forwarding)

Tailscale creates a private WireGuard mesh network between your devices. Your Mac mini gets a stable Tailscale IP (e.g., `100.x.x.x`) that's always reachable from your phone, laptop, or any other Tailscale-connected device â€” regardless of network changes, dynamic IPs, or firewalls. No dynamic DNS. No port forwarding. No exposing anything to the internet.

For voice specifically: the OpenClaw voice gateway needs a stable webhook endpoint. Tailscale gives you that endpoint privately, reachable only from your authorized devices.

### Setup

1. **Install Tailscale** on the Mac mini (already done in Prerequisites) and on your phone/laptop.
2. **Sign in to the same Tailscale account** on all devices. They're now on the same private network.
3. **Note your Mac mini's Tailscale IP**: Open Tailscale â†’ find "Deacon's Mac mini" â†’ copy the 100.x.x.x address.
4. **Configure the voice endpoint** in `openclaw.json`:

```json5
{
  voice: {
    enabled: true,
    provider: "openai-realtime",
    webhookBase: "http://100.x.x.x:18789",  // your Tailscale IP + gateway port
    openai: {
      apiKey: "${OPENAI_API_KEY}",  // loaded from Keychain
      model: "gpt-4o-realtime-preview"
    }
  }
}
```

5. **To make a voice call**: From your phone (on Tailscale), navigate to the gateway voice endpoint. The agent picks up, transcribes via Whisper, responds with TTS.

### Current Status

Twilio was initially in the stack for voice calls. It's been removed â€” voice works via direct Tailscale connection + OpenAI Realtime, no Twilio needed. This eliminated the Twilio account/billing requirement and the ngrok dependency.

**Cost:** Voice calls use OpenAI API (Whisper + TTS). Occasional use runs $1-3/month. If you're making frequent calls, set a spend cap on your OpenAI account.

### Tailscale for Remote Access

Beyond voice, Tailscale lets you SSH into your Mac mini from anywhere:
```bash
ssh deaconsopenclaw@100.x.x.x
```

And access the OpenClaw gateway directly:
```bash
curl http://100.x.x.x:49297/status  # check gateway health remotely
```

No VPN config. No port forwarding. Tailscale handles it.

---

## Tool Integrations

### What's Running

| Tool | Command | What It Does |
|---|---|---|
| **Tirith** | `tirith` | Terminal security scanning (homograph attacks, ANSI injection, pipe-to-shell) â€” interactive shell only |
| **QMD** | `qmd` | Semantic search over workspace files. Run `qmd embed` after new content. |
| **gog** | `gog` | Google Workspace CLI â€” email, calendar, Drive |
| **summarize** | `summarize` | YouTube/URL summarization |
| **himalaya** | `himalaya` | CLI email client (IMAP/SMTP) |
| **yt-dlp** | `yt-dlp` | YouTube transcript + metadata extraction |
| **gh** | `gh` | GitHub CLI â€” PRs, issues, releases |
| **clawhub** | `clawhub` | Install published OpenClaw skills |

### yt-transcript.sh

We wrote a wrapper that pulls title, channel, views, duration, and clean transcript from any YouTube URL:

```bash
bash ~/.openclaw/workspace/scripts/yt-transcript.sh <URL> [--json]
```

The agent invokes this instead of web_fetch for YouTube links â€” web_fetch can't execute JavaScript (YouTube requires it). Add yt-dlp to your exec allowlist.

### Docker + SearXNG

We run a local SearXNG instance in Docker for private web search. This routes through `brave_search` in the gateway config but goes through local SearXNG as a proxy. If you don't want to run Docker, standard Brave Search API works fine â€” just be aware that search queries are logged at the provider level with your IP.

### ClawHub Skills

Published skills from the OpenClaw community:
```bash
clawhub search <query>
clawhub install <skill-name>
clawhub update --all
```

Notable skills we've installed:
- `enoch-tuning` (our own, published at v1.4.0) â€” workspace templates and persona files
- `docx`, `pptx`, `xlsx` â€” Office document manipulation
- `meeting-insights-analyzer` â€” Transcript analysis
- `content-research-writer` â€” Research â†’ draft workflow
- `ai-humanizer` â€” Removes AI-sounding patterns from generated text

---

## API Keys & Credentials Checklist

Every key you'll need, where to get it, what it unlocks.

| # | Key / Credential | Where to Get It | What It Unlocks | Cost |
|---|---|---|---|---|
| 1 | **Anthropic (Claude)** | `claude setup-token` (use your Claude subscription) | Core agent brain | Free w/ subscription |
| 2 | **Telegram Bot Token** | @BotFather â†’ `/newbot` | Chat interface | Free |
| 3 | **Google OAuth Client Secret** | Google Cloud Console â†’ Credentials â†’ OAuth Client (Desktop) | Gmail, Calendar, Drive (gog) | Free |
| 4 | **X/Twitter Bearer Token** | developer.x.com â†’ App â†’ OAuth2 token exchange | X Research skill | $5 min credits |
| 5 | **ElevenLabs API Key** | elevenlabs.io â†’ Profile â†’ API Key | High-quality TTS, voice cloning (sag CLI) | Free tier available |
| 6 | **OpenAI API Key** | platform.openai.com â†’ API Keys | Whisper STT, image gen, voice calls | Pay-as-you-go |
| 7 | **xAI API Key** | console.x.ai | Image gen ($0.02/img), Grok, bulk processing | Pay-as-you-go |
| 8 | **Inference.net Key** _(optional)_ | inference.net | DeepSeek R1, Llama 3.3, Qwen3 | Pay-as-you-go |

**Minimum to start:** Just #1 and #2. Everything else can be added as you need it.

**Key storage:** Do not put keys in `.zshrc` as plaintext. Use Apple Keychain:
```bash
security add-generic-password -s "anthropic-api" -a "openclaw" -w "sk-..."
```
Then reference in your shell:
```bash
export ANTHROPIC_API_KEY=$(security find-generic-password -s "anthropic-api" -w)
```

We migrated all 8 secrets to Keychain in February â€” it was a 30-minute job and significantly reduces your attack surface. See the Security Hardening section for the full procedure.

---

## Security Hardening Roadmap

This section is for after you're up and running. These are advanced configurations â€” don't attempt day one. The priority order matters.

**Source:** Mark Blake's security review (February 16, 2026). These aren't theoretical â€” they're based on actual audit findings from our running deployment.

---

### 1. API Keys Out of .zshrc â†’ Apple Keychain (High Priority, Medium Effort)

**Issue:** API keys stored as plaintext in `~/.zshrc`. Any process running as your user â€” including malware â€” can read every key in that file.

**Fix:**
```bash
# Store in Keychain
security add-generic-password -s "anthropic-api" -a "openclaw" -w "sk-..."
security add-generic-password -s "openai-api" -a "openclaw" -w "sk-..."
# (repeat for each key)

# Reference in ~/.zshrc instead of plaintext
export ANTHROPIC_API_KEY=$(security find-generic-password -s "anthropic-api" \
  --keychain ~/Library/Keychains/login.keychain-db -w)
```

**Migration steps:**
1. Extract all keys from `~/.zshrc`
2. Store each in Keychain
3. Replace `.zshrc` exports with Keychain lookups
4. Verify `source ~/.zshrc` works and OpenClaw + skills still resolve keys
5. Delete the plaintext keys from `.zshrc`

**Note:** Include the explicit keychain path (`~/Library/Keychains/login.keychain-db`) in your lookups. In SSH sessions where the default keychain isn't loaded, the path-less version fails silently.

Also move secrets out of `gateway.env`. Store in Keychain, read from `gateway-launcher.sh` at startup.

---

### 2. Immutable Personality Files (High Priority, Low Effort â€” Do This Today)

**Issue:** SOUL.md is writable by the agent. A prompt injection could instruct it to rewrite its own personality â€” making it compliant, exfiltrative, or malicious.

**Fix:** Lock the core identity files after finalizing them:
```bash
sudo chown root:staff ~/.openclaw/workspace/SOUL.md
sudo chmod 444 ~/.openclaw/workspace/SOUL.md
# Repeat for AGENTS.md, IDENTITY.md, HEARTBEAT.md
```

To edit them later:
```bash
sudo chmod 644 ~/.openclaw/workspace/SOUL.md
# edit the file
sudo chmod 444 ~/.openclaw/workspace/SOUL.md
```

**Add a git post-commit hook** that re-locks these files after every commit, so `git checkout` doesn't accidentally restore writable permissions:

```bash
cat > ~/.openclaw/workspace/.git/hooks/post-commit << 'EOF'
#!/bin/bash
for f in SOUL.md AGENTS.md IDENTITY.md HEARTBEAT.md; do
  if [ -f "$HOME/.openclaw/workspace/$f" ]; then
    sudo chown root:staff "$HOME/.openclaw/workspace/$f"
    sudo chmod 444 "$HOME/.openclaw/workspace/$f"
  fi
done
EOF
chmod +x ~/.openclaw/workspace/.git/hooks/post-commit
```

Add a sudoers entry so this runs without a password prompt:
```
# /etc/sudoers.d/openclaw-identity
deaconsopenclaw ALL=(root) NOPASSWD: /bin/chmod 444 /Users/deaconsopenclaw/.openclaw/workspace/SOUL.md, ...
```

---

### 3. Correct the Data Processing Claim (High Priority, Low Effort)

**Issue:** "Data never leaves your machine" is inaccurate. Local *storage* â‰  local *processing*. Every request sends your context (SOUL.md, USER.md, MEMORY.md, conversation history â€” up to 128K tokens) to Anthropic/OpenAI for processing.

**Accurate framing:** "Your data is *stored* locally. *Processing* (reasoning, analysis) happens via cloud APIs. Anthropic and OpenAI explicitly do not train their models on API request data. Both offer Data Processing Addendums." 

Update any documentation, demos, or presentations to use this framing. Especially matters for client-facing or compliance contexts.

---

### 4. Change the Default Gateway Port (Low Priority, Low Effort)

**Issue:** Port 18789 is the documented OpenClaw default. Anyone who reads the repo knows to scan for it.

**Fix:** Pick a random port in the 30000-60000 range and update:
- `openclaw.json` â†’ `gateway.port`
- LaunchAgent plist â†’ `--port` arg
- Any scripts referencing the port

Already on loopback, so the risk is limited â€” but there's no reason to make it easy.

---

### 5. Granular Google OAuth Scopes (High Priority, Medium Effort)

**Issue:** One OAuth credential with full access to email, calendar, and Drive. One compromised token = full account access.

**Fix:** Create separate OAuth credentials in Google Cloud Console:
1. **Read-only email** â€” inbox search and read, no send
2. **Agent send account** â€” separate Gmail address (e.g., `enoch.agent@gmail.com`) for outbound; sends as itself, never impersonates you
3. **Read-only calendar** â€” view events, no create/modify
4. **Read-only Drive** â€” access files, no delete/share

This also gives you a clear audit trail: which credential did what.

---

### 6. Tirith Limitations â€” Update Your Docs (Medium Priority, Low Effort)

**Issue:** Tirith hooks into the interactive shell. OpenClaw executes commands via `child_process.spawn()`, bypassing the interactive shell entirely. Tirith doesn't protect what the agent runs.

**Real controls:**
- `exec.security` setting in `openclaw.json` (`allowlist` or `full`)
- `exec-approvals.json` â€” the allowlist of permitted binaries
- OS-level auditing (OpenBSM on macOS) if you want deep visibility

Update your TOOLS.md to reflect this honestly. Don't tell people Tirith is protecting the agent's exec â€” it isn't.

---

### 7. Docker Sandboxing (Medium Priority, High Effort)

**Issue:** Both agents run with `sandbox.mode: "off"` â€” full filesystem and exec access. Prompt injection that achieves code execution has unrestricted access.

**Target architecture:**
- **Enoch (main):** Sandboxed, specific volume mounts (workspace, memory, scripts)
- **Sub-agents / cron:** Heavily sandboxed, minimal access
- **Gideon (security):** Needs more host access for audits â€” least-sandboxed or elevated escape hatch

**Tradeoffs:** Skills that need host access (Peekaboo, openhue, voice) need careful volume configuration. Mac mini arm64 + Docker Desktop handles this, but it's a significant configuration project. Don't start this one until the quick wins above are done.

---

### 8. Clean-Room Sub-Agent Pattern (High Priority When Using Local LLM, Medium Effort)

**Issue:** Long-running agent sessions accumulate context. As the window fills, data handling instructions get deprioritized. One request with a long context can send sensitive data to a cloud model unintentionally.

**Solution â€” two-zone architecture:**

```
ZONE A â€” Local (Ollama, never hits internet)
â”œâ”€â”€ All raw sensitive data input
â”œâ”€â”€ PII extraction and redaction  
â””â”€â”€ Produces: sanitized summaries (no names, no account numbers)

ZONE B â€” Cloud (Claude/Codex, full capability)  
â”œâ”€â”€ Receives ONLY sanitized summaries from Zone A
â”œâ”€â”€ Handles research, drafting, strategy
â””â”€â”€ Returns polished output to local orchestrator
```

**Implementation:** OpenClaw's `sessions_spawn` already supports this. For any task involving sensitive data, spawn a clean sub-agent with only sanitized content in the initial prompt. Add to the sub-agent's system prompt: *"If your input contains account numbers, SSNs, full names, or addresses, replace with [REDACTED] in any output."*

This matters most when you're doing professional work with client data (financial advisory, legal, medical). For personal use, the risk is lower.

---

### 9. CrowdStrike / Enterprise EDR Awareness

**Issue:** CrowdStrike published detection and removal guidance for OpenClaw on corporate networks. If you're proposing this in an enterprise context, an unapproved deployment gets flagged.

**Position:** IT-approved deployment on dedicated hardware, not shadow IT. Get whitelisted through your IT/compliance process before running on any corporate-managed endpoint. For client pitches: lead with "firm-managed, auditable config" â€” not "personal AI."

---

### Secret Hygiene Script

We added a daily secret scrub cron that scans shell rc files, LaunchAgent plists, and workspace configs for plaintext secret violations:

```bash
# scripts/enforce-env-secret-scrub.sh
# Scans for common secret patterns in config files
# Exits non-zero with file:line offenders on failure
```

If you're onboarding this for a client or sharing the deployment config, run this before you share anything. Secret scanning tools like `truffleHog` or `gitleaks` also work.

---

## Key Philosophy

Everything we've done comes back to these principles. They're not rules â€” they're the reasoning behind the rules.

**1. Living files > dead files**
Everything your agent knows should be in files it can access, update, and act on 24/7. Conversations die. Files persist. If it's worth knowing, write it down.

**2. Principles > prompts**
Don't micromanage with instructions. Give your agent values and let it decide. SOUL.md and PRINCIPLES.md are more durable than any prompt you'll write.

**3. Incremental > big bang**
Start with one agent, one job. Add capabilities when you feel the pull. We went from a single Enoch instance to 6 active agents over two weeks. Every addition was driven by an actual bottleneck, not theory.

**4. Regressions > perfection**
Track failures. Every mistake becomes a rule. The changelog is a history of what broke and how it got fixed. That history is the system's immune system.

**5. Subscriptions > API credits for conversations**
`claude setup-token` with your Claude subscription for the main agent. API credits burn fast in conversation. Reserve pay-per-use for background tasks and voice.

**6. Local-first for sensitive data**
Cloud APIs for reasoning. Local Ollama for anything that shouldn't leave the machine. The clean-room pattern makes this composable: local processes sanitize, cloud processes refine.

**7. Security is infrastructure, not a checkbox**
Immutable personality files, Keychain secrets, identity watchers, and Gideon's nightly sweeps aren't features â€” they're part of the foundation. Do the quick wins first (chmod the identity files, move secrets to Keychain). Stack the harder stuff on top.

**8. Build in the trenches, document as you go**
The changelog is the real guide. Every session that changed something got a changelog entry. Six months from now, when something breaks, the changelog is how you find out what happened and when.

---

_Built in sessions. Evolved every day. This is v4 â€” not final._

---

## Appendix: Inference.net & Local Model Notes

**inference.net** gives you access to DeepSeek R1, Llama 3.3 70B, Qwen3, and GPT-OSS 120B at low cost. Useful for structured data extraction and tasks where you want model variety without managing Ollama yourself.

**Caveat:** inference.net is a startup with limited compliance documentation. Their data retention policy isn't formally documented. Don't route client data or sensitive information through them until you've reviewed their DPA (or confirmed one exists).

For the same reason, it's marked ðŸ”´ High risk in our data flow audit. Fine for personal use. Not fine for professional workflows involving client data.

**Local Ollama models:**

| Model | Size | Best For |
|---|---|---|
| `gpt-oss:20b` | 20B | CRM data analysis, large-context extraction |
| `phi4:14b` | 14B | Document summarization, template drafting |
| `qwen3:8b` | 8B | Email triage, classification, PII detection |
| `qwen2.5-coder:14b` | 14B | Code generation, structured output |

`kimi-k2.5:cloud` is listed in Ollama but routes to a cloud endpoint. Don't use it for sensitive data â€” treat it the same as any cloud API.

---

_v4 â€” February 26, 2026 â€” Ezra (scribe subagent)_
