<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Apple Silicon Local AI Buyer's Guide: Mac mini to Mac Studio | Ridley Research & Consulting</title>
  <meta name="description" content="Every M4 Mac tier from $599 to $3,999+, the free models each one can run, and how to pick the right machine for local AI operations." />
  <meta property="og:title" content="Apple Silicon Local AI Buyer's Guide: Mac mini to Mac Studio | Ridley Research & Consulting" />
  <meta property="og:description" content="Every M4 Mac tier from $599 to $3,999+, the free models each one can run, and how to pick the right machine for local AI operations." />
  <meta property="og:image" content="https://ridleyresearch.com/rr-mark.png" />
  <meta property="og:url" content="https://ridleyresearch.com/blog/apple-silicon-local-ai-buyers-guide" />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Apple Silicon Local AI Buyer's Guide: Mac mini to Mac Studio | Ridley Research & Consulting" />
  <meta name="twitter:description" content="Every M4 Mac tier from $599 to $3,999+, the free models each one can run, and how to pick the right machine for local AI operations." />
  <meta name="twitter:image" content="https://ridleyresearch.com/rr-mark.png" />

  <link rel="stylesheet" href="../styles.css?v=9" />
  <link rel="icon" href="/rr-favicon.png" type="image/png" />
  <style>
    .tier-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 20px;
      margin: 40px 0;
    }
    .tier-card {
      background: rgba(255,255,255,0.04);
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 16px;
      padding: 24px;
      position: relative;
      overflow: hidden;
    }
    .tier-card::before {
      content: "";
      position: absolute;
      top: 0; left: 0; right: 0;
      height: 3px;
    }
    .tier-entry::before  { background: linear-gradient(90deg, #4ade80, #22c55e); }
    .tier-mid::before    { background: linear-gradient(90deg, #5b8fff, #3d6eff); }
    .tier-pro::before    { background: linear-gradient(90deg, #a855f7, #7c3aed); }
    .tier-max::before    { background: linear-gradient(90deg, #f59e0b, #d97706); }
    .tier-ultra::before  { background: linear-gradient(90deg, #ef4444, #dc2626); }

    .tier-label {
      font-size: 11px;
      font-weight: 700;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      margin-bottom: 8px;
      font-family: "Space Grotesk", Inter, sans-serif;
    }
    .tier-entry  .tier-label { color: #4ade80; }
    .tier-mid    .tier-label { color: #7eb8ff; }
    .tier-pro    .tier-label { color: #c084fc; }
    .tier-max    .tier-label { color: #fbbf24; }
    .tier-ultra  .tier-label { color: #f87171; }

    .tier-name {
      font-family: "Space Grotesk", Inter, sans-serif;
      font-size: 20px;
      font-weight: 700;
      letter-spacing: -0.02em;
      margin: 0 0 4px;
    }
    .tier-price {
      font-size: 13px;
      color: #8494c8;
      margin-bottom: 16px;
    }
    .spec-row {
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      padding: 7px 0;
      border-bottom: 1px solid rgba(255,255,255,0.06);
      font-size: 13px;
      gap: 12px;
    }
    .spec-row:last-of-type { border-bottom: none; }
    .spec-key { color: #8494c8; flex-shrink: 0; }
    .spec-val { color: #e8ecff; font-weight: 500; text-align: right; }
    .models-section {
      margin-top: 16px;
      padding-top: 16px;
      border-top: 1px solid rgba(255,255,255,0.08);
    }
    .models-label {
      font-size: 11px;
      font-weight: 700;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: #8494c8;
      margin-bottom: 10px;
    }
    .model-tag {
      display: inline-block;
      background: rgba(91,143,255,0.12);
      border: 1px solid rgba(91,143,255,0.2);
      border-radius: 6px;
      padding: 3px 8px;
      font-size: 12px;
      margin: 3px 3px 3px 0;
      color: #c8d8ff;
    }
    .schematic {
      background: rgba(255,255,255,0.03);
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 16px;
      padding: 28px 32px;
      margin: 40px 0;
      overflow-x: auto;
    }
    .schematic-title {
      font-family: "Space Grotesk", Inter, sans-serif;
      font-size: 13px;
      font-weight: 700;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: #8494c8;
      margin-bottom: 20px;
    }
    .schema-row {
      display: flex;
      align-items: center;
      gap: 16px;
      padding: 10px 0;
      border-bottom: 1px solid rgba(255,255,255,0.05);
      font-size: 13px;
      flex-wrap: wrap;
    }
    .schema-row:last-child { border-bottom: none; }
    .schema-machine { width: 160px; flex-shrink: 0; font-weight: 600; color: #e8ecff; font-family: "Space Grotesk", Inter, sans-serif; }
    .schema-ram { width: 80px; flex-shrink: 0; color: #7eb8ff; font-weight: 700; }
    .schema-bar-wrap { flex: 1; min-width: 120px; background: rgba(255,255,255,0.06); border-radius: 4px; height: 8px; }
    .schema-bar { height: 8px; border-radius: 4px; }
    .schema-models { color: #8494c8; font-size: 12px; min-width: 200px; }
    .callout {
      background: linear-gradient(135deg, rgba(91,143,255,0.08), rgba(30,50,140,0.1));
      border: 1px solid rgba(91,143,255,0.2);
      border-radius: 12px;
      padding: 20px 24px;
      margin: 32px 0;
    }
    .callout strong { color: #7eb8ff; }
    @media (max-width: 600px) {
      .tier-grid { grid-template-columns: 1fr; }
      .schematic { padding: 20px 16px; }
    }
  </style>
</head>
<body>
  <header class="nav">
    <div class="nav-inner">
      <a href="/" style="text-decoration:none;color:inherit;" class="brand-link">
        <img src="/rr-mark.png" alt="RR" style="height:32px;width:32px;object-fit:cover;border-radius:2px;display:block;" />
      </a>
      <div class="nav-links">
        <div class="nav-dropdown" id="ocDropdown">
          <button class="nav-dropdown-btn" onclick="toggleDropdown('ocDropdown')">
            Menu <span class="chevron">▾</span>
          </button>
          <div class="nav-dropdown-menu">
            <div class="dropdown-section-label">Explore</div>
            <a href="/about">About</a>
            <a href="/blog/">Blog</a>
            <a href="/openclaw/what-is-openclaw">OpenClaw &#8594;</a>
            <a href="/testimonials/submit">Leave a Review</a>
            <div class="dropdown-section-label">Work With Us</div>
            <a href="/small-business/">Small Business &#8594;</a>
            <a href="/products/">Products &amp; Pricing</a>
            <a href="mailto:hello@ridleyresearch.com?subject=Discovery%20Call">Book a Discovery Call</a>
          </div>
        </div>
      </div>
    </div>
  </header>
  <main class="wrap">
    <article class="post">
      <h1>Apple Silicon Local AI Buyer's Guide: Mac mini to Mac Studio</h1>
      <p class="muted">Published: 2026-02-25 · 8 min read</p>

      <p>Running AI models locally is no longer a hobbyist move. With Apple Silicon's unified memory architecture, even a $599 Mac mini can run capable open-source models — no GPU rig required. This guide maps every current Apple desktop tier to the free models it can actually run, so you can buy the right machine instead of the most expensive one.</p>

      <p>All specs are current as of early 2026 (M4 Mac mini, M4 Max / M4 Ultra Mac Studio). All models listed are free and runnable via <a href="https://ollama.com" style="color:var(--accent)">Ollama</a>.</p>

      <div class="callout">
        <strong>Why unified memory matters:</strong> On Apple Silicon, the CPU, GPU, and Neural Engine share one pool of high-bandwidth memory. That means a 36GB Mac Studio can load a 34B parameter model entirely into memory — something a PC with a 24GB GPU can't do. The memory number is the whole ballgame.
      </div>

      <h2>The Full Tier Breakdown</h2>

      <div class="tier-grid">

        <!-- Tier 1: Mac mini M4 base -->
        <div class="tier-card tier-entry">
          <div class="tier-label">Entry</div>
          <div class="tier-name">Mac mini M4</div>
          <div class="tier-price">From $599</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">10-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">10-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">16GB (↑32GB)</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">120 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">256GB–2TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (16GB)</div>
            <span class="model-tag">Llama 3.2 3B</span>
            <span class="model-tag">Phi-4 mini</span>
            <span class="model-tag">Gemma 3 4B</span>
            <span class="model-tag">Mistral 7B (Q4)</span>
            <span class="model-tag">Qwen2.5 7B</span>
            <span class="model-tag">DeepSeek-R1 7B</span>
          </div>
        </div>

        <!-- Tier 2: Mac mini M4 32GB -->
        <div class="tier-card tier-entry">
          <div class="tier-label">Entry+</div>
          <div class="tier-name">Mac mini M4 32GB</div>
          <div class="tier-price">From $799 (upgraded)</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">10-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">10-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">32GB</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">120 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">256GB–2TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (32GB)</div>
            <span class="model-tag">Llama 3.1 8B</span>
            <span class="model-tag">Mistral 7B (full)</span>
            <span class="model-tag">Phi-4 14B</span>
            <span class="model-tag">Gemma 3 12B</span>
            <span class="model-tag">CodeLlama 13B</span>
            <span class="model-tag">DeepSeek-R1 14B</span>
            <span class="model-tag">Qwen2.5 14B</span>
          </div>
        </div>

        <!-- Tier 3: Mac mini M4 Pro 24GB -->
        <div class="tier-card tier-mid">
          <div class="tier-label">Mid</div>
          <div class="tier-name">Mac mini M4 Pro</div>
          <div class="tier-price">From $1,399</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Pro</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">14-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">20-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">24GB (↑48GB)</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">273 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">512GB–4TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (24GB)</div>
            <span class="model-tag">Llama 3.1 8B</span>
            <span class="model-tag">Phi-4 14B (Q4)</span>
            <span class="model-tag">Gemma 3 12B</span>
            <span class="model-tag">DeepSeek-R1 14B</span>
            <span class="model-tag">Qwen2.5 14B</span>
            <span class="model-tag">Mixtral 8x7B (Q2)</span>
          </div>
        </div>

        <!-- Tier 4: Mac mini M4 Pro 48GB -->
        <div class="tier-card tier-pro">
          <div class="tier-label">Pro</div>
          <div class="tier-name">Mac mini M4 Pro 48GB</div>
          <div class="tier-price">From $1,599 (upgraded)</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Pro</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">14–16-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">20–24-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">48GB</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">273 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">512GB–4TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (48GB)</div>
            <span class="model-tag">Llama 3.3 70B (Q4)</span>
            <span class="model-tag">Qwen2.5 32B</span>
            <span class="model-tag">DeepSeek-R1 32B</span>
            <span class="model-tag">Mixtral 8x7B (Q4)</span>
            <span class="model-tag">CodeLlama 34B</span>
            <span class="model-tag">Phi-4 medium</span>
          </div>
        </div>

        <!-- Tier 5: Mac Studio M4 Max 36GB -->
        <div class="tier-card tier-max">
          <div class="tier-label">Max</div>
          <div class="tier-name">Mac Studio M4 Max</div>
          <div class="tier-price">From $1,999</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Max</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">14-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">32-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">36GB (↑64GB, ↑128GB)</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">546 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">512GB–8TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (36GB)</div>
            <span class="model-tag">Llama 3.1 70B (Q4 tight)</span>
            <span class="model-tag">Qwen2.5 32B (full)</span>
            <span class="model-tag">DeepSeek-R1 32B (full)</span>
            <span class="model-tag">Mixtral 8x7B</span>
            <span class="model-tag">CodeLlama 34B</span>
          </div>
        </div>

        <!-- Tier 6: Mac Studio M4 Max 64GB -->
        <div class="tier-card tier-max">
          <div class="tier-label">Max+</div>
          <div class="tier-name">Mac Studio M4 Max 64GB</div>
          <div class="tier-price">From $2,399</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Max</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">16-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">40-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">64GB (↑128GB)</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">546 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">512GB–8TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">16-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (64GB)</div>
            <span class="model-tag">Llama 3.1 70B (Q8)</span>
            <span class="model-tag">Qwen2.5 72B (Q4)</span>
            <span class="model-tag">DeepSeek-R1 70B (Q4)</span>
            <span class="model-tag">Mixtral 8x22B (Q4)</span>
            <span class="model-tag">Llama 3.1 405B (Q2)</span>
          </div>
        </div>

        <!-- Tier 7: Mac Studio M4 Ultra 192GB -->
        <div class="tier-card tier-ultra">
          <div class="tier-label">Ultra</div>
          <div class="tier-name">Mac Studio M4 Ultra</div>
          <div class="tier-price">From $3,999</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Ultra</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">28-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">60-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">192GB (↑512GB)</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">800 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">1TB–16TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">32-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (192GB)</div>
            <span class="model-tag">Llama 3.1 405B (Q4)</span>
            <span class="model-tag">Qwen2.5 72B (full)</span>
            <span class="model-tag">DeepSeek-R1 671B (Q2)</span>
            <span class="model-tag">Mixtral 8x22B (full)</span>
            <span class="model-tag">Any sub-100B model (full)</span>
          </div>
        </div>

        <!-- Tier 8: Mac Studio M4 Ultra 512GB -->
        <div class="tier-card tier-ultra">
          <div class="tier-label">Ultra Max</div>
          <div class="tier-name">Mac Studio M4 Ultra 512GB</div>
          <div class="tier-price">From $7,999</div>
          <div class="spec-row"><span class="spec-key">Chip</span><span class="spec-val">Apple M4 Ultra</span></div>
          <div class="spec-row"><span class="spec-key">CPU Cores</span><span class="spec-val">28-core</span></div>
          <div class="spec-row"><span class="spec-key">GPU Cores</span><span class="spec-val">80-core</span></div>
          <div class="spec-row"><span class="spec-key">Unified Memory</span><span class="spec-val">512GB</span></div>
          <div class="spec-row"><span class="spec-key">Memory BW</span><span class="spec-val">800 GB/s</span></div>
          <div class="spec-row"><span class="spec-key">Storage</span><span class="spec-val">1TB–16TB SSD</span></div>
          <div class="spec-row"><span class="spec-key">Neural Engine</span><span class="spec-val">32-core</span></div>
          <div class="models-section">
            <div class="models-label">Free Models It Runs (512GB)</div>
            <span class="model-tag">Llama 3.1 405B (full)</span>
            <span class="model-tag">DeepSeek-R1 671B (Q4+)</span>
            <span class="model-tag">Every open model available</span>
            <span class="model-tag">Multi-model parallel serving</span>
          </div>
        </div>

      </div>

      <h2>Memory vs. Model Size: The Visual Map</h2>
      <p>The single most important number is unified memory. Here's how it maps to what you can actually run:</p>

      <div class="schematic">
        <div class="schematic-title">RAM → Model Capability Map</div>

        <div class="schema-row">
          <div class="schema-machine">Mac mini M4 / 16GB</div>
          <div class="schema-ram">16 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:9%;background:linear-gradient(90deg,#4ade80,#22c55e)"></div></div>
          <div class="schema-models">Up to ~7B params (Q4) — good for fast, focused tasks</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac mini M4 / 32GB</div>
          <div class="schema-ram">32 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:18%;background:linear-gradient(90deg,#4ade80,#22c55e)"></div></div>
          <div class="schema-models">Up to ~14B params — capable reasoning, coding</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac mini M4 Pro / 24GB</div>
          <div class="schema-ram">24 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:14%;background:linear-gradient(90deg,#5b8fff,#3d6eff)"></div></div>
          <div class="schema-models">Up to ~14B params — faster inference than base M4</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac mini M4 Pro / 48GB</div>
          <div class="schema-ram">48 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:28%;background:linear-gradient(90deg,#a855f7,#7c3aed)"></div></div>
          <div class="schema-models">Up to ~34B params (Q4) — runs 70B quantized</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac Studio M4 Max / 36GB</div>
          <div class="schema-ram">36 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:21%;background:linear-gradient(90deg,#f59e0b,#d97706)"></div></div>
          <div class="schema-models">Up to ~32B (full) — 70B tight at Q4</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac Studio M4 Max / 64GB</div>
          <div class="schema-ram">64 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:37%;background:linear-gradient(90deg,#f59e0b,#d97706)"></div></div>
          <div class="schema-models">Up to 70B (Q8) — frontier open models</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac Studio M4 Max / 128GB</div>
          <div class="schema-ram">128 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:62%;background:linear-gradient(90deg,#f59e0b,#d97706)"></div></div>
          <div class="schema-models">405B at Q2/Q3 — multi-model stacking</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac Studio M4 Ultra / 192GB</div>
          <div class="schema-ram">192 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:80%;background:linear-gradient(90deg,#ef4444,#dc2626)"></div></div>
          <div class="schema-models">405B at Q4 — DeepSeek-R1 671B at Q2</div>
        </div>

        <div class="schema-row">
          <div class="schema-machine">Mac Studio M4 Ultra / 512GB</div>
          <div class="schema-ram">512 GB</div>
          <div class="schema-bar-wrap"><div class="schema-bar" style="width:100%;background:linear-gradient(90deg,#ef4444,#dc2626)"></div></div>
          <div class="schema-models">Every open-weight model — full precision</div>
        </div>
      </div>

      <h2>Which Tier Should You Actually Buy?</h2>

      <div class="callout">
        <strong>For most operators:</strong> Mac mini M4 Pro with 48GB is the sweet spot. $1,599 gets you 70B-class models, fast inference via the M4 Pro's 273 GB/s memory bandwidth, and a machine that won't become obsolete for years. The base 16GB mini is fine for lightweight automation — not for serious agent work.
      </div>

      <p>Here's the honest breakdown by use case:</p>

      <ul>
        <li><strong>Personal automation, simple agents, light coding:</strong> Mac mini M4 16GB ($599) — Llama 3.2 3B and Phi-4 mini handle most day-to-day tasks cleanly.</li>
        <li><strong>Coding assistant, research agent, document work:</strong> Mac mini M4 32GB or M4 Pro 24GB ($799–$1,399) — 14B-class models give you real reasoning without hitting limits.</li>
        <li><strong>Serious multi-agent ops, 70B-class reasoning:</strong> Mac mini M4 Pro 48GB ($1,599) — this is the floor for running Llama 3.3 70B and DeepSeek-R1 32B without compromise.</li>
        <li><strong>Production inference server, parallel model serving:</strong> Mac Studio M4 Max 64GB ($2,399+) — the 546 GB/s bandwidth means real throughput, not just capacity.</li>
        <li><strong>Research lab, full-precision frontier models:</strong> Mac Studio M4 Ultra ($3,999+) — the only desktop that runs 405B and 671B models usably.</li>
      </ul>

      <h2>The Free Model Stack (via Ollama)</h2>
      <p>All models below run locally via <a href="https://ollama.com" style="color:var(--accent)">Ollama</a> on macOS. Zero API costs, zero data leaving your machine.</p>
      <ul>
        <li><strong>Llama 3.1 / 3.2 / 3.3</strong> — Meta's flagship open series. 8B, 70B, 405B. Best all-around.</li>
        <li><strong>DeepSeek-R1</strong> — 7B, 14B, 32B, 70B, 671B. Exceptional reasoning. Chinese open weights.</li>
        <li><strong>Phi-4 / Phi-4 mini</strong> — Microsoft. Punches above weight at small sizes. Great for low-memory machines.</li>
        <li><strong>Qwen2.5</strong> — Alibaba. 7B, 14B, 32B, 72B. Strong coding and instruction following.</li>
        <li><strong>Gemma 3</strong> — Google. 4B, 12B, 27B. Good balance of speed and quality.</li>
        <li><strong>Mixtral 8x7B / 8x22B</strong> — Mistral's mixture-of-experts. Fast at inference relative to parameter count.</li>
        <li><strong>CodeLlama</strong> — Meta's code-focused variant. 7B, 13B, 34B, 70B.</li>
      </ul>

      <h2>Getting Started</h2>
      <p>Install Ollama, pull a model, and you're running local AI in under 5 minutes:</p>
      <pre style="background:rgba(0,0,0,0.3);border:1px solid rgba(255,255,255,0.08);border-radius:10px;padding:16px;overflow-x:auto;font-size:13px;line-height:1.6;"><code style="color:#c8d8ff"># Install Ollama
brew install ollama

# Pull and run a model
ollama run llama3.2        # 3B — fast, 16GB+
ollama run phi4            # 14B — smart, 32GB+
ollama run llama3.3:70b    # 70B — frontier, 48GB+
ollama run deepseek-r1:32b # 32B reasoning, 48GB+</code></pre>

      <p>Once running, Ollama exposes an OpenAI-compatible API at <code style="color:#7eb8ff">localhost:11434</code> — drop it into any tool that accepts an OpenAI endpoint.</p>

      <div class="callout">
        <strong>The case for local AI:</strong> No API bills. No rate limits. No data leaving your machine. For operators handling sensitive client data — financial, legal, medical — local models aren't optional, they're the only defensible choice. A $1,599 Mac mini M4 Pro pays for itself in 2–3 months vs. equivalent API usage at scale.
      </div>

      <p style="margin-top:40px">Questions about building a local AI stack for your team? <a href="mailto:hello@ridleyresearch.com" style="color:var(--accent)">Reach out.</a></p>
    </article>

    <p class="foot">© <span id="y"></span> Ridley Research &amp; Consulting. All rights reserved.</p>
  </main>
  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
  <script src="/chat-widget.js?v=2"></script>
  <script>
    const revealEls = document.querySelectorAll(".tier-card, .schematic, .callout");
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(e => {
        if (e.isIntersecting) {
          e.target.style.opacity = "1";
          e.target.style.transform = "translateY(0)";
          observer.unobserve(e.target);
        }
      });
    }, { threshold: 0.05 });
    revealEls.forEach((el, i) => {
      el.style.opacity = "0";
      el.style.transform = "translateY(20px)";
      el.style.transition = `opacity 0.5s ease ${i * 0.06}s, transform 0.5s ease ${i * 0.06}s`;
      observer.observe(el);
    });
  </script>
</body>
</html>
